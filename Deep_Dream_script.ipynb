{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JitheshPavan/deep_dream/blob/main/Deep_Dream_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gusz1l445mr5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Image processing\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numbers\n",
        "\n",
        "from tqdm import tqdm\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vOwr_DrIK1e",
        "outputId": "3d26f60a-1e01-48b3-c9f8-d87ec843daf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'deep_dream'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 99 (delta 28), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (99/99), 2.97 MiB | 7.39 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/JitheshPavan/deep_dream.git\n",
        "\n",
        "!wget https://github.com/JitheshPavan/deep_dream/blob/main/data/lion.jpg?raw=true -O lion.jpg -q\n",
        "!wget https://github.com/JitheshPavan/deep_dream/blob/main/data/figures.jpg?raw=true -O figures.jpg -q\n",
        "!wget https://github.com/JitheshPavan/deep_dream/blob/main/data/me.jpg?raw=true -O .jpg -q\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('deep_dream/utilities')\n",
        "\n",
        "from utility import *\n",
        "\n",
        "\n",
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gjAAZPUduK4"
      },
      "source": [
        "# DEEP DREAM ALGORITHM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzY9Qh0jojYS"
      },
      "outputs": [],
      "source": [
        "def deepdream_image_generator(PIL_image,model):\n",
        "\n",
        "  tensor_image= pre_prcoess_PIL_to_np_to_tensor(PIL_image) # (B,C,H,W)\n",
        "  # tensor_image.requires_grad=True # user created tensors has to be manually set this. But we do not need this here. If we start the graph from here backward will compute gradient till here.\n",
        "  image_sizes= pyramid_ratio_generator(tensor_image,number_of_pyramids,pyramid_ratio)\n",
        "\n",
        "  for i in tqdm(image_sizes, desc=\"Processing Pyramid Levels\", bar_format=\"{l_bar}{bar} [Elapsed: {elapsed}]\"):\n",
        "\n",
        "    tensor_image=transforms.Resize(i)(tensor_image)\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "      h_shift, w_shift = np.random.randint(-shift, shift + 1, 2)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        tensor_image = torch.roll(tensor_image, (h_shift,w_shift),(2,3))\n",
        "\n",
        "      gradient_ascent(tensor_image,model)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        tensor_image = torch.roll(tensor_image,(-h_shift, -w_shift),(2,3)) # this will output a leaf no requires_grad tensor breaking the computational graph connection to previous tensors\n",
        "\n",
        "  return post_process_torch_to_numpy(tensor_image)\n",
        "\n",
        "def gradient_ascent(tensor_image,model):\n",
        "  tensor_image.requires_grad=True# the grads will fill till here. Retain_grad has to be called as method. requires_grad is called as func\n",
        "  intermediate_activations= model.forward(tensor_image) # Therefore image must be set True for Two reasons 1) Model parameters are set requiers_grad =False so we need to do this to calculate gradients otherwise gradients will not be calulcated. 2)we are updating the image with gradients.\n",
        "  if intermediate_activations[0].requires_grad==False:\n",
        "    raise ValueError(\" Intermediate activations do not have requires_grad true\")\n",
        "  loss=[]\n",
        "  for i in intermediate_activations:\n",
        "    loss.append(i.sum())\n",
        "  loss=torch.stack(loss)\n",
        "  loss= torch.sum(loss)\n",
        "  loss.backward()\n",
        "  grad= tensor_image.grad.data\n",
        "  sigma = (iterations + 1) / iterations * 2.0 + 0.5\n",
        "  smooth_grad = CascadeGaussianSmoothing(kernel_size=9, sigma=sigma,DEVICE=device)(grad)\n",
        "  # print(f'data/grad={abs(tensor_image.std()/ smooth_grad.std())}')\n",
        "  # lr=  10**-10 * abs(tensor_image.std()/ smooth_grad.std())\n",
        "  with torch.no_grad():\n",
        "    tensor_image += lr * smooth_grad    # An inplace operation. This inplace operation is only allowed using  torch.no_grad().\n",
        "  tensor_image.grad.zero_()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnkgeynjdx8t"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZLnABCVqgK2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PretrainedModel(nn.Module):\n",
        "    def __init__(self, model, layer_names_or_indices, is_sequential=False):\n",
        "        super(PretrainedModel, self).__init__()\n",
        "        self.model = model\n",
        "        self.layer_names_or_indices = layer_names_or_indices\n",
        "        self.is_sequential = is_sequential\n",
        "        self.feature_maps = [None] * len(layer_names_or_indices)\n",
        "\n",
        "        # Register hooks for the appropriate layers based on the model type\n",
        "        if self.is_sequential:\n",
        "            # For Sequential models, treat layer_names_or_indices as indices\n",
        "            for i, layer_idx in enumerate(self.layer_names_or_indices):\n",
        "                layer = self.model.features[layer_idx]  # Access Sequential layers by index\n",
        "                layer.register_forward_hook(self.create_hook(i))\n",
        "        else:\n",
        "            # For named layers, treat layer_names_or_indices as layer names\n",
        "            for i, layer_name in enumerate(self.layer_names_or_indices):\n",
        "                layer = getattr(self.model, layer_name)  # Access named layers\n",
        "                layer.register_forward_hook(self.create_hook(i))\n",
        "\n",
        "        # Freeze model parameters\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def create_hook(self, index):\n",
        "        def hook_fn(module, input, output):\n",
        "            self.feature_maps[index] = output\n",
        "        return hook_fn\n",
        "\n",
        "    def forward(self, image):\n",
        "        # Forward pass through the model\n",
        "        _ = self.model(image)\n",
        "\n",
        "        # Return the feature maps captured by the hooks as a tuple\n",
        "        return tuple(self.feature_maps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMj33dDqfU9S",
        "outputId": "091cb78b-0ef1-4520-e0c8-a8d2e3d04ab2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(cached_file)\n"
          ]
        }
      ],
      "source": [
        "# from facenet_pytorch import InceptionResnetV1, MTCNN\n",
        "\n",
        "# # Load the FaceNet model (Inception ResNet v1)\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # Initialize the FaceNet model\n",
        "# facenet = InceptionResnetV1(pretrained='vggface2',classify=True).eval().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHgm7v-pfZqc"
      },
      "source": [
        "# Final"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "shift=32 # How image is to be shifted during roll.\n",
        "pyramid_ratio =1.8\n",
        "number_of_pyramids = 4\n",
        "iterations=10\n",
        "lr= 0.005 # for VGG\n",
        "# lr=0.5 # For facenet\n",
        "# layer_name=('conv2d_4b', 'mixed_7a') # Facenet\n",
        "layer_name = (22,) # VGG16\n",
        "model = models.vgg16(weights= 'VGG16_Weights.IMAGENET1K_V1')\n",
        "image=deepdream_image_generator(Image.open('mantis.jpg'),PretrainedModel(model,layer_name, is_sequential= True)) # for facenet sequential is not true\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "lDV4zoZdwccc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82kkXzodRfAu"
      },
      "outputs": [],
      "source": [
        "PIL= image_pil = Image.fromarray(image)\n",
        "PIL.save('jithesh_output.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMs6ECX3pdVT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-gjAAZPUduK4",
        "tnkgeynjdx8t"
      ],
      "authorship_tag": "ABX9TyMZc9WIWum2l1K+ag7STJGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}